class System:
    """
    A Python class containing the vision model and the specific neuron to interact with.
    
    Attributes
    ----------
    neuron_num : int
        The unit number of the neuron.
    layer : string
        The name of the layer where the neuron is located.
    model_name : string
        The name of the vision model.
    model : nn.Module
        The loaded PyTorch model.
    device : torch.device
        The device (CPU/GPU) used for computations.

    Methods
    -------
    call_neuron(image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]
        Returns the neuron activation for each image in the input image_list as well as the activation map 
        of the neuron over that image, that highlights the regions of the image where the activations 
        are higher (encoded into a Base64 string).
    """
    
    def __init__(self, neuron_num: int, layer: str, model_name: str, device: str):
        """
        Initializes a neuron object by specifying its number and layer location and the vision model that the neuron belongs to.
        Parameters
        -------
        neuron_num : int
            The unit number of the neuron.
        layer : str
            The name of the layer where the neuron is located.
        model_name : str
            The name of the vision model that the neuron is part of.
        device : str
            The computational device ('cpu' or 'cuda').
        """
        self.neuron_num = neuron_num
        self.layer = layer
        self.device = torch.device(f"cuda:{device}" if torch.cuda.is_available() else "cpu")   
    
    def call_neuron(self, image_list: List[torch.Tensor]) -> Tuple[List[float], List[str]]:
        """
        The function returns the neuron’s maximum activation value (in int format) for each of the images in the list as well as the activation map of the neuron over each of the images that highlights the regions of the image where the activations are higher (encoded into a Base64 string).
        
        Parameters
        ----------
        image_list : List[torch.Tensor]
            The input image
        
        Returns
        -------
        Tuple[List[int], List[str]]
            For each image in image_list returns the activation value of the neuron on that image, and a masked image, 
            with the region of the image that caused the high activation values highlighted (and the rest of the image is darkened). Each image is encoded into a Base64 string.
        
        Examples
        --------
        >>> # test the activation of the neuron for the prompt "a dog standing on the grass"
        >>> prompt = ["a dog standing on the grass"]
        >>> image = tools.text2image(prompt)
        >>> activation_list, activation_map_list = system.call_neuron(image)
        >>> for activation, image in zip(activation_list, activation_map_list):
        >>>     tools.display(image, f"Activation: {activation}")
        >>>
        >>> # test the activation of the neuron for the prompt "a dog standing on the grass" and maintain robustness to noise
        >>> prompts = ["a dog standing on the grass"]*5
        >>> images = tools.text2image(prompts)
        >>> activation_list, activation_map_list = system.call_neuron(images)
        >>> tools.display(activation_map_list[0], f"Activation: {statistics.mean(activation_list)}")
        >>>
        >>> # test the activation of the neuron for the prompt "a landscape with a tree and river"
        >>> # for the same image but with different seasons:
        >>> prompts = ["a landscape with a tree and a river"]*3
        >>> original_images = tools.text2image(prompts)
        >>> edits = ["make it autumn","make it spring","make it winter"]
        >>> all_images, all_prompts = tools.edit_images(original_images, edits)
        >>> activation_list, activation_map_list = system.call_neuron(all_images)
        >>> for activation, image, prompt in zip(activation_list, activation_map_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nActivation: {activation}")
        """

class Tools:
    """
    A Python class containing tools to interact with the units implemented in the system class, 
    in order to run experiments on it.

    Attributes
    ----------
    text2image_model_name : str
        The name of the text-to-image model.
    text2image_model : any
        The loaded text-to-image model.
    path2save : str
        Path for saving output images.
    threshold : any
        Activation threshold for neuron analysis.
    device : torch.device
        The device (CPU/GPU) used for computations.
    experiment_log: str
        A log of all the experiments, including the code and the output from the neuron
        analysis.
    exemplars : Dict
        A dictionary containing the exemplar images for each unit.
    exemplars_activations : Dict
        A dictionary containing the activations for each exemplar image.
    exemplars_thresholds : Dict
        A dictionary containing the threshold values for each unit.
    results_list : List
        A list of the results from the neuron analysis.

    Methods
    -------
    dataset_exemplars(system: System)->List[Tuple[int, str]]
        This experiment provides good coverage of the behavior observed on a
        very large dataset of images and therefore represents the typical
        behavior of the neuron on real images. This function characterizes the
        prototypical behavior of the neuron by computing its activation on 
        all images in the ImageNet dataset and returning the 15 highest activations 
        and the images that produced them in Base64 encoded string format.
    edit_images(self, base_images: List[str], editing_prompts: List[str]) -> Tuple[List[str], List[str]]
        This function enables localized testing of specific hypotheses about how
        variations on the content of a single image affect neuron activations.
        Gets a list of input images in Base64 encoded string format and a list of 
        corresponding editing instructions, then edits each provided image based on the 
        instructions given in the prompt using a text-based image editing model. The 
        function returns a list of images in Base64 encoded string format and list of the 
        relevant prompts. This function is very useful for testing the causality of the 
        neuron in a controlled way, or example by testing how the neuron activation 
        is affected by changing one aspect of the image. IMPORTANT: Do not use negative 
        terminology such as "remove ...", try to use terminology like "replace ... with ..." 
        or "change the color of ... to ...".
    text2image(prompt_list: str) -> List[str]
        Gets a list of text prompts as an input and generates an image for each
        prompt using a text to image model. The function returns a
        list of images in Base64 encoded string format.
    summarize_images(self, image_list: List[str]) -> str:    
        This function is useful to summarize the mutual visual concept that
        appears in a set of images. It gets a list of images at input and
        describes what is common to all of them.
    describe_images(synthetic_image_list: List[str], synthetic_image_title:List[str]) -> str
        Provides impartial descriptions of images. Do not use this function on
        dataset exemplars. Gets a list of images and generates a textual
        description of the semantic content of each of them.
        The function is blind to the current hypotheses list and
        therefore provides an unbiased description of the visual content.
    display(self, *args: Union[str, Image.Image]):
        This function is your way of displaying experiment data. You must call
        this on results/variables that you wish to view in order to view them. 
    """    

    def __init__(self, path2save: str, device: str, DatasetExemplars: DatasetExemplars = None, text2image_model_name='sd'):
        """
        Initializes the Tools object.

        Parameters
        ----------
        path2save : str
            Path for saving output images.
        device : str
            The computational device ('cpu' or 'cuda').
        DatasetExemplars : object
            an object from the class DatasetExemplars
        text2image_model_name : str
            The name of the text-to-image model.
        """

    def dataset_exemplars(self, system: System) -> List[Tuple[float, str]]
        """
        This method finds images from the ImageNet dataset that produce the highest activations for a specific neuron.
        It returns both the activations and the corresponding exemplar images that were used to generate these activations.
        This experiment is performed on real images and will provide a good approximation of the neuron behavior.
        
        Parameters
        ----------
        system : System
            The system representing the specific neuron and layer within the neural network.
            The system should have 'layer' and 'neuron_num' attributes, so the dataset_exemplars function 
            can return the exemplar activations and images for that specific neuron.

        Returns
        -------
        List
            For each exemplar image, stores a tuple containing two elements:
            - The first element is the activation for the specified neuron.
            - The second element is the exemplar images (as Base64 encoded strings) corresponding to the activation.

        Example
        -------
        >>> exemplar_data = tools.dataset_exemplars(system)
        >>> for activation, image in exemplar_data:
        >>>    tools.display(image, f"Activation: {activation}")
        """
        
    def edit_images(self,
                    base_images: List[str],
                    editing_prompts: List[str]) -> Tuple[List[str], List[str]]:
        """
        Generates or uses provided base images, then edits each base image with a
        corresponding editing prompt. Accepts either text prompts or Base64
        encoded strings as sources for the base images.

        The function returns a list containing lists of images (original and edited,
        interleaved) in Base64 encoded string format, and a list of the relevant
        prompts (original source string and editing prompt, interleaved).

        Parameters
        ----------
        base_images : List[str]
            A list of images as Base64 encoded strings. These images are to be 
            edited by the prompts in editing_prompts.
        editing_prompts : List[str]
            A list of instructions for how to edit the base images derived from
            `base_images`. Must be the same length as `base_images`.

        Returns
        -------
        Tuple[List[str], List[str]]
            - all_images: A list where elements alternate between:
                - A Base64 string for the original image from a source.
                - A Base64 string for the edited image from that source.
              Example: [orig1_img1, edit1_img1, orig2_img1, edit2_img1, ...]
            - all_prompts: A list where elements alternate between:
                - The original source string (text prompt or Base64) used.
                - The editing prompt used.
              Example: [source1, edit1, source2, edit2, ...]
            The order in `all_images` corresponds to the order in `all_prompts`.

        Raises
        ------
        ValueError
            If the lengths of `base_images` and `editing_prompts` are not equal.

        Examples
        --------
        >>> # test the activation of the neuron for the prompt "a landscape with a tree and river"
        >>> # for the same image but with different seasons:
        >>> prompts = ["a landscape with a tree and a river"]*3
        >>> original_images = tools.text2image(prompts)
        >>> edits = ["make it autumn","make it spring","make it winter"]
        >>> all_images, all_prompts = tools.edit_images(original_images, edits)
        >>> activation_list, activation_map_list = system.call_neuron(all_images)
        >>> for activation, image, prompt in zip(activation_list, activation_map_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nActivation: {activation}")
        >>> 
        >>> # test the activation of the neuron on the highest scoring dataset exemplar
        >>> # under different conditions        
        >>> exemplar_data = tools.dataset_exemplars(system)
        >>> highest_scoring_exemplar = exemplar_data[0][1]
        >>> edits = ["make it night","make it daytime","make it snowing"]
        >>> all_images, all_prompts = tools.edit_images([highest_scoring_exemplar]*len(edits), edits)
        >>> activation_list, activation_map_list = system.call_neuron(all_images)
        >>> for activation, image, prompt in zip(activation_list, activation_map_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nActivation: {activation}")
        """

    def text2image(self, prompt_list: List[str]) -> List[str]:
        """
        Takes a list of text prompts and generates an image for each using a
        text to image model. The function returns a list of images.

        Parameters
        ----------
        prompt_list : List[str]
            A list of text prompts for image generation.

        Returns
        -------
        List[str]
            A list of images in Base64 encoded string format for each input prompts. 


        Examples
        --------
        >>> # Generate images from a list of prompts
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> activation_list, activation_map_list = system.call_neuron(images)
        >>> for activation, image in zip(activation_list, activation_map_list):
        >>>     tools.display(image, f"Activation: {activation}")
        """

    def display(self, *args: Union[str, Image.Image]):
        """
        Displays a series of images and/or text in the chat, similar to a Jupyter notebook.
        
        Parameters
        ----------
        *args : Union[str, Image.Image]
            The content to be displayed in the chat. Can be multiple strings or Image objects.

        Notes
        -------
        Displays directly to chat interface.

        Example
        -------
        >>> # Display a single image
        >>> prompt = ["a dog standing on the grass"]
        >>> images = tools.text2image(prompt)
        >>> activation_list, activation_map_list = system.call_neuron(images)
        >>> for activation, image in zip(activation_list, activation_map_list):
        >>>     tools.display(image, f"Activation: {activation}")
        >>>
        >>> # Display a single image from a list
        >>> prompts = ["a dog standing on the grass"]*5
        >>> images = tools.text2image(prompts)
        >>> activation_list, activation_map_list = system.call_neuron(images)
        >>> tools.display(activation_map_list[0], f"Activation: {statistics.mean(activation_list)}")
        >>>
        >>> # Display a list of images
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> activation_list, activation_map_list = system.call_neuron(images)
        >>> for activation, image in zip(activation_list, activation_map_list):
        >>>     tools.display(image, f"Activation: {activation}")
        """

    def summarize_images(self, image_list: List[str]) -> str:
        """
        Gets a list of images and describes what is common to all of them.


        Parameters
        ----------
        image_list : list
            A list of images in Base64 encoded string format.
        
        Returns
        -------
        str
            A string with a descriptions of what is common to all the images.

        Example
        -------
        >>> # Summarize a neuron's dataset exemplars
        >>> exemplars = [exemplar for _, exemplar in tools.dataset_exemplars(system)] # Get exemplars
        >>> summarization = tools.summarize_images(exemplars)
        >>> tools.display(summarization)
        """

    def describe_images(self, image_list: List[str], image_title:List[str]) -> str:
        """
        Generates textual descriptions for a list of images, focusing
        specifically on highlighted regions. The final descriptions are
        concatenated and returned as a single string, with each description
        associated with the corresponding image title.

        Parameters
        ----------
        image_list : List[str]
            A list of images in Base64 encoded string format.
        image_title : List[str]
            A list of titles for each image in the image_list.

        Returns
        -------
        str
            A concatenated string of descriptions for each image, where each description 
            is associated with the image's title and focuses on the highlighted regions 
            in the image.

        Example
        -------
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> activation_list, activation_map_list = system.call_neuron(images)
        >>> descriptions = tools.describe_images(activation_map_list, prompt_list)
        >>> tools.display(descriptions)
        """